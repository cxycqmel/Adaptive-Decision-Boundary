{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "loaded-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import torch\n",
    "import copy\n",
    "import random\n",
    "import csv\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "from datetime import datetime\n",
    "from transformers import PreTrainedModel, BertTokenizer, BertModel, AdamW, BertConfig, BertForSequenceClassification\n",
    "from tqdm import tqdm_notebook, trange, tqdm\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "# from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "WEIGHTS_NAME = 'pytorch_model.bin'\n",
    "CONFIG_NAME = 'config.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-cricket",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "grateful-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "    \n",
    "    \n",
    "args = {\n",
    "    \"data_dir\": '/fred/oz064/xcai/ADB/Adaptive-Decision-Boundary/data',\n",
    "    \"save_results_path\": 'outputs',\n",
    "    \"pretrain_dir\": 'models',\n",
    "    \"bert_model\": \"/fred/oz064/xcai/pytorch/huggingface/bert-base-uncased\",\n",
    "    \"max_seq_length\": None,\n",
    "    \"feat_dim\": 768,\n",
    "    \"warmup_proportion\": 0.1,\n",
    "    \"freeze_bert_parameters\": True,\n",
    "    \"save_model\": True,\n",
    "    \"save_results\": True,\n",
    "    \"dataset\": \"oos\",\n",
    "    \"known_cls_ratio\": 0.75,\n",
    "    \"labeled_ratio\": 1.0,\n",
    "    \"method\": None,\n",
    "    \"seed\": 0,\n",
    "    \"gpu_id\": '0',\n",
    "    \"lr\": 2e-5,\n",
    "    \"num_train_epochs\": 100.0,\n",
    "    \"train_batch_size\": 128,\n",
    "    \"eval_batch_size\": 64,\n",
    "    \"wait_patient\": 10,\n",
    "    \"lr_boundary\": 0.05,\n",
    "    \"num_labels\": 10,\n",
    "}\n",
    "args = dotdict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "outstanding-three",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/fred/oz064/xcai/ADB/Adaptive-Decision-Boundary/data'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-puppy",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "entire-asian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "class Data:\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        set_seed(args.seed)\n",
    "        max_seq_lengths = {'oos':30, 'stackoverflow':45,'banking':55}\n",
    "        args.max_seq_length = max_seq_lengths[args.dataset]\n",
    "\n",
    "        processor = DatasetProcessor()\n",
    "        self.data_dir = os.path.join(args.data_dir, args.dataset)\n",
    "        self.all_label_list = processor.get_labels(self.data_dir)\n",
    "        self.n_known_cls = round(len(self.all_label_list) * args.known_cls_ratio)\n",
    "        self.known_label_list = list(np.random.choice(np.array(self.all_label_list), self.n_known_cls, replace=False))\n",
    "\n",
    "        self.num_labels = len(self.known_label_list)\n",
    "        \n",
    "        if args.dataset == 'oos':\n",
    "            self.unseen_token = 'oos'\n",
    "        else:\n",
    "            self.unseen_token = '<UNK>'\n",
    "        \n",
    "        self.unseen_token_id = self.num_labels\n",
    "        self.label_list = self.known_label_list + [self.unseen_token]\n",
    "        self.train_examples = self.get_examples(processor, args, 'train')\n",
    "        self.eval_examples = self.get_examples(processor, args, 'eval')\n",
    "        self.test_examples = self.get_examples(processor, args, 'test')\n",
    "        \n",
    "        self.train_dataloader = self.get_loader(self.train_examples, args, 'train')\n",
    "        self.eval_dataloader = self.get_loader(self.eval_examples, args, 'eval')\n",
    "        self.test_dataloader = self.get_loader(self.test_examples, args, 'test')\n",
    "        \n",
    "    def get_examples(self, processor, args, mode = 'train'):\n",
    "        ori_examples = processor.get_examples(self.data_dir, mode)\n",
    "        \n",
    "        examples = []\n",
    "        if mode == 'train':\n",
    "            for example in ori_examples:\n",
    "                if (example.label in self.known_label_list) and (np.random.uniform(0, 1) <= args.labeled_ratio):\n",
    "                    examples.append(example)\n",
    "        elif mode == 'eval':\n",
    "            for example in ori_examples:\n",
    "                if (example.label in self.known_label_list):\n",
    "                    examples.append(example)\n",
    "        elif mode == 'test':\n",
    "            for example in ori_examples:\n",
    "                if (example.label in self.label_list) and (example.label is not self.unseen_token):\n",
    "                    examples.append(example)\n",
    "                else:\n",
    "                    example.label = self.unseen_token\n",
    "                    examples.append(example)\n",
    "        return examples\n",
    "    \n",
    "    def get_loader(self, examples, args, mode = 'train'):\n",
    "        tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=True)    \n",
    "        features = convert_examples_to_features(examples, self.label_list, args.max_seq_length, tokenizer)\n",
    "        input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "        input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "        segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "        label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "        datatensor = TensorDataset(input_ids, input_mask, segment_ids, label_ids)\n",
    "        \n",
    "        if mode == 'train':\n",
    "            sampler = RandomSampler(datatensor)\n",
    "            dataloader = DataLoader(datatensor, sampler=sampler, batch_size = args.train_batch_size)    \n",
    "        elif mode == 'eval' or mode == 'test':\n",
    "            sampler = SequentialSampler(datatensor)\n",
    "            dataloader = DataLoader(datatensor, sampler=sampler, batch_size = args.eval_batch_size) \n",
    "        \n",
    "        return dataloader\n",
    "\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                if sys.version_info[0] == 2:\n",
    "                    line = list(unicode(cell, 'utf-8') for cell in line)\n",
    "                lines.append(line)\n",
    "            return lines\n",
    "\n",
    "class DatasetProcessor(DataProcessor):\n",
    "\n",
    "    def get_examples(self, data_dir, mode):\n",
    "        if mode == 'train':\n",
    "            return self._create_examples(\n",
    "                self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "        elif mode == 'eval':\n",
    "            return self._create_examples(\n",
    "                self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"train\")\n",
    "        elif mode == 'test':\n",
    "            return self._create_examples(\n",
    "                self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        import pandas as pd\n",
    "        test = pd.read_csv(os.path.join(data_dir, \"train.tsv\"), sep=\"\\t\")\n",
    "        labels = np.unique(np.array(test['label']))\n",
    "            \n",
    "        return labels\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            if len(line) != 2:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[0]\n",
    "            label = line[1]\n",
    "\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "        return examples\n",
    "\n",
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "    label_map = {}\n",
    "    for i, label in enumerate(label_list):\n",
    "        label_map[label] = i\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens += tokens_b + [\"[SEP]\"]\n",
    "            segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        label_id = label_map[example.label]\n",
    "        # if ex_index < 5:\n",
    "        #     logger.info(\"*** Example ***\")\n",
    "        #     logger.info(\"guid: %s\" % (example.guid))\n",
    "        #     logger.info(\"tokens: %s\" % \" \".join(\n",
    "        #         [str(x) for x in tokens]))\n",
    "        #     logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "        #     logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "        #     logger.info(\n",
    "        #         \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "        #     logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(input_ids=input_ids,\n",
    "                          input_mask=input_mask,\n",
    "                          segment_ids=segment_ids,\n",
    "                          label_id=label_id))\n",
    "    return features\n",
    "\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop(0)  # For dialogue context\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "            \n",
    "data = Data(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-sheep",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "decent-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForModel(nn.Module):\n",
    "    config_class = BertConfig\n",
    "    base_model_prefix = \"bert\"\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = BertConfig(args.bert_model)\n",
    "        self.bert = BertModel.from_pretrained(args.bert_model)\n",
    "        self.dense = nn.Linear(self.config.hidden_size, self.config.hidden_size)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(self.config.hidden_size, self.num_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "        \n",
    "    def init_bert_weights(self, module):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, input_ids = None, token_type_ids = None, attention_mask=None , labels = None,\n",
    "                feature_ext = False, mode = None, centroids = None):\n",
    "\n",
    "        output = self.bert(input_ids, token_type_ids, attention_mask)\n",
    "#         pooled_output = self.dense(encoded_layer_12[-1].mean(dim = 1))\n",
    "        pooled_output = self.dense(output.pooler_output)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        if feature_ext:\n",
    "            return pooled_output\n",
    "        else:\n",
    "            if mode == 'train':\n",
    "                loss = nn.CrossEntropyLoss()(logits,labels)\n",
    "                return loss\n",
    "            else:\n",
    "                return pooled_output, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-programming",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "chubby-humanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_metric(a, b):\n",
    "    n = a.shape[0]\n",
    "    m = b.shape[0]\n",
    "    a = a.unsqueeze(1).expand(n, m, -1)\n",
    "    b = b.unsqueeze(0).expand(n, m, -1)\n",
    "    logits = -((a - b)**2).sum(dim=2)\n",
    "    return logits\n",
    "\n",
    "class BoundaryLoss(nn.Module):\n",
    "    def __init__(self, num_labels=10, feat_dim=2):\n",
    "        super(BoundaryLoss, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.feat_dim = feat_dim\n",
    "        self.delta = nn.Parameter(torch.randn(num_labels).cuda())\n",
    "        nn.init.normal_(self.delta)\n",
    "        \n",
    "    def forward(self, pooled_output, centroids, labels):\n",
    "        logits = euclidean_metric(pooled_output, centroids)\n",
    "        probs, preds = F.softmax(logits.detach(), dim=1).max(dim=1) \n",
    "        delta = F.softplus(self.delta)\n",
    "        c = centroids[labels]\n",
    "        d = delta[labels]\n",
    "        x = pooled_output\n",
    "        \n",
    "        euc_dis = torch.norm(x - c,2, 1).view(-1)\n",
    "        pos_mask = (euc_dis > d).type(torch.cuda.FloatTensor)\n",
    "        neg_mask = (euc_dis < d).type(torch.cuda.FloatTensor)\n",
    "\n",
    "        pos_loss = (euc_dis - d) * pos_mask\n",
    "        neg_loss = (d - euc_dis) * neg_mask\n",
    "        loss = pos_loss.mean() + neg_loss.mean()\n",
    "        \n",
    "        return loss, delta "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-respondent",
   "metadata": {},
   "source": [
    "# Pretrain Model Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "acting-milton",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainModelManager:\n",
    "    \n",
    "    def __init__(self, args, data):\n",
    "\n",
    "        print(args.bert_model)\n",
    "        args.num_labels = data.num_labels\n",
    "        self.model = BertForModel(args)\n",
    "#         .from_pretrained(\n",
    "#             args.bert_model, \n",
    "#             num_labels=data.num_labels,\n",
    "#             cache_dir=\"\")\n",
    "        if args.freeze_bert_parameters:\n",
    "            for name, param in self.model.bert.named_parameters():  \n",
    "                param.requires_grad = False\n",
    "                if \"encoder.layer.11\" in name or \"pooler\" in name:\n",
    "                    param.requires_grad = True\n",
    "                    \n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_id           \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "        if n_gpu > 1:\n",
    "            self.model = torch.nn.DataParallel(self.model)\n",
    "        \n",
    "        self.num_train_optimization_steps = int(len(data.train_examples) / args.train_batch_size) * args.num_train_epochs\n",
    "        \n",
    "        self.optimizer = self.get_optimizer(args)\n",
    "        \n",
    "        self.best_eval_score = 0\n",
    "\n",
    "    def eval(self, args, data):\n",
    "        \n",
    "        self.model.eval()\n",
    "\n",
    "        total_labels = torch.empty(0,dtype=torch.long).to(self.device)\n",
    "        total_logits = torch.empty((0, data.num_labels)).to(self.device)\n",
    "        \n",
    "        for batch in data.eval_dataloader:\n",
    "            batch = tuple(t.to(self.device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, label_ids = batch\n",
    "            with torch.set_grad_enabled(False):\n",
    "                _, logits = self.model(input_ids, segment_ids, input_mask, mode = 'eval')\n",
    "                total_labels = torch.cat((total_labels,label_ids))\n",
    "                total_logits = torch.cat((total_logits, logits))\n",
    "        \n",
    "        total_probs, total_preds = F.softmax(total_logits.detach(), dim=1).max(dim = 1)\n",
    "        y_pred = total_preds.cpu().numpy()\n",
    "        y_true = total_labels.cpu().numpy()\n",
    "        acc = round(accuracy_score(y_true, y_pred) * 100, 2)\n",
    "\n",
    "        return acc\n",
    "\n",
    "\n",
    "    def train(self, args, data):    \n",
    "\n",
    "        wait = 0\n",
    "        best_model = None\n",
    "        for epoch in range(int(args.num_train_epochs)):\n",
    "            print(\"epoch \", epoch)\n",
    "            self.model.train()\n",
    "            tr_loss = 0\n",
    "            nb_tr_examples, nb_tr_steps = 0, 0\n",
    "            \n",
    "            for step, batch in enumerate(data.train_dataloader):\n",
    "                batch = tuple(t.to(self.device) for t in batch)\n",
    "                input_ids, input_mask, segment_ids, label_ids = batch\n",
    "                with torch.set_grad_enabled(True):\n",
    "                    loss = self.model(input_ids, segment_ids, input_mask, label_ids, mode = \"train\")\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    \n",
    "                    tr_loss += loss.item()\n",
    "                    nb_tr_examples += input_ids.size(0)\n",
    "                    nb_tr_steps += 1\n",
    "            \n",
    "            loss = tr_loss / nb_tr_steps\n",
    "            print('train_loss',loss)\n",
    "            \n",
    "            eval_score = self.eval(args, data)\n",
    "            print('eval_score',eval_score)\n",
    "            \n",
    "            if eval_score > self.best_eval_score:\n",
    "                best_model = copy.deepcopy(self.model)\n",
    "                wait = 0\n",
    "                self.best_eval_score = eval_score\n",
    "            else:\n",
    "                wait += 1\n",
    "                if wait >= args.wait_patient:\n",
    "                    break\n",
    "                \n",
    "        self.model = best_model\n",
    "        if args.save_model:\n",
    "            self.save_model(args)\n",
    "\n",
    "    def get_optimizer(self, args):\n",
    "\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                         lr = args.lr)   \n",
    "        return optimizer\n",
    "    \n",
    "    def save_model(self, args):\n",
    "\n",
    "        if not os.path.exists(args.pretrain_dir):\n",
    "            os.makedirs(args.pretrain_dir)\n",
    "        self.save_model = self.model.module if hasattr(self.model, 'module') else self.model  \n",
    "\n",
    "        model_file = os.path.join(args.pretrain_dir, WEIGHTS_NAME)\n",
    "        model_config_file = os.path.join(args.pretrain_dir, CONFIG_NAME)\n",
    "        torch.save(self.save_model.state_dict(), model_file)\n",
    "        with open(model_config_file, \"w\") as f:\n",
    "            f.write(self.save_model.config.to_json_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-participant",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training begin...\n",
      "/fred/oz064/xcai/pytorch/huggingface/bert-base-uncased\n",
      "epoch  0\n",
      "train_loss 4.717719316482544\n",
      "eval_score 2.54\n",
      "epoch  1\n",
      "train_loss 4.684949072924527\n",
      "eval_score 2.01\n",
      "epoch  2\n",
      "train_loss 4.589606165885925\n",
      "eval_score 1.96\n",
      "epoch  3\n",
      "train_loss 4.5046911077065905\n",
      "eval_score 2.77\n",
      "epoch  4\n",
      "train_loss 4.418477253480391\n",
      "eval_score 2.72\n",
      "epoch  5\n",
      "train_loss 4.327202119610527\n",
      "eval_score 4.06\n",
      "epoch  6\n",
      "train_loss 4.184024323116649\n",
      "eval_score 4.51\n",
      "epoch  7\n",
      "train_loss 4.0629157315601\n",
      "eval_score 5.67\n",
      "epoch  8\n",
      "train_loss 3.959718457677148\n",
      "eval_score 6.83\n",
      "epoch  9\n",
      "train_loss 3.8572674962607296\n",
      "eval_score 9.15\n",
      "epoch  10\n",
      "train_loss 3.751717830246145\n",
      "eval_score 10.36\n",
      "epoch  11\n",
      "train_loss 3.648626601154154\n",
      "eval_score 10.49\n",
      "epoch  12\n",
      "train_loss 3.558861041610891\n",
      "eval_score 11.88\n",
      "epoch  13\n",
      "train_loss 3.477258029309186\n",
      "eval_score 14.46\n",
      "epoch  14\n",
      "train_loss 3.4106947590004313\n",
      "eval_score 16.12\n",
      "epoch  15\n",
      "train_loss 3.334947504780509\n",
      "eval_score 17.86\n",
      "epoch  16\n",
      "train_loss 3.2592224695465783\n",
      "eval_score 17.95\n",
      "epoch  17\n"
     ]
    }
   ],
   "source": [
    "print('Pre-training begin...')\n",
    "manager_p = PretrainModelManager(args, data)\n",
    "manager_p.train(args, data)\n",
    "print('Pre-training finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "democratic-spine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "lovely-mercury",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /fred/oz064/xcai/pytorch/huggingface/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BertForSequenceClassification.from_pretrained(args.bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "satisfied-society",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2df73b6d115e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m BertForModel.from_pretrained(\n\u001b[1;32m      5\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             cache_dir=\"\")\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# for e in data.train_examples:\n",
    "#     print(e.text_a, \"\\t\", e.label)\n",
    "# data.num_labels\n",
    "BertForModel.from_pretrained(\n",
    "            args.bert_model, \n",
    "            num_labels=data.num_labels,\n",
    "            cache_dir=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-martial",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
